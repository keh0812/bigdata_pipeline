{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사내 서버 카프카에 \"json_topic\" 토픽 추가 하였다, 없으면 추가\n",
    "# {\"id\":1,\"firstname\":\"James \",\"middlename\":\"\",\"lastname\":\"Smith\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":3000}\n",
    "# {\"id\":2,\"firstname\":\"Michael \",\"middlename\":\"Rose\",\"lastname\":\"\",\"dob_year\":2010,\"dob_month\":3,\"gender\":\"M\",\"salary\":4000}\n",
    "# {\"id\":3,\"firstname\":\"Robert \",\"middlename\":\"\",\"lastname\":\"Williams\",\"dob_year\":2010,\"dob_month\":3,\"gender\":\"M\",\"salary\":4000}\n",
    "# {\"id\":4,\"firstname\":\"Maria \",\"middlename\":\"Anne\",\"lastname\":\"Jones\",\"dob_year\":2005,\"dob_month\":5,\"gender\":\"F\",\"salary\":4000}\n",
    "# {\"id\":5,\"firstname\":\"Jen\",\"middlename\":\"Mary\",\"lastname\":\"Brown\",\"dob_year\":2010,\"dob_month\":7,\"gender\":\"\",\"salary\":-1}\n",
    "\n",
    "#setting\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "import nb_init\n",
    "\n",
    "os.environ['CEP_ENV'] = 'local'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 Pyspark 모듈 import\n",
    "from cep_common import spark_utils as su\n",
    "from cep_common import config as con\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = su.create_spark_session(appName=\"kafka-sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스파크 스트리밍으로 카프카 데이터 읽어오기\n",
    "topic_name = \"json_topic\"\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", con.KAFKA_NODES) \\\n",
    "  .option(\"subscribe\", topic_name) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df에서 value값만 가져와서 binary에서 string 형태로 캐스팅한다\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#커스텀 스키마 형태\n",
    "schema = StructType().add(\"id\", \"integer\") \\\n",
    "      .add(\"firstname\", \"string\") \\\n",
    "      .add(\"middlename\",\"string\") \\\n",
    "      .add(\"lastname\",\"string\") \\\n",
    "      .add(\"dob_year\",\"integer\") \\\n",
    "      .add(\"dob_month\",\"integer\") \\\n",
    "      .add(\"gender\",\"string\") \\\n",
    "      .add(\"salary\",\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df에서 value 값을 스키마 형태로 변형\n",
    "df = df.select(F.from_json(df.value, schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df = df.withColumn(\"lastname_starting_char\", F.substring(F.col(\"lastname\"), 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x15f26511d88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delta 형태로 저장 Lastname 첫 알파벳으로 파티션한다\n",
    "save_path = \"hdfs://{}/datalake/kafka/topics/{}\".format(con.HADOOP_NODE, topic_name)\n",
    "\n",
    "df.writeStream \\\n",
    "      .format(\"delta\") \\\n",
    "      .outputMode(\"append\") \\\n",
    "      .partitionBy(\"lastname_starting_char\") \\\n",
    "      .trigger(processingTime='1 seconds') \\\n",
    "      .option(\"checkpointLocation\", save_path+\"/check_points\") \\\n",
    "      .option(\"path\", save_path) \\\n",
    "      .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+--------+---------+------+------+----------------------+\n",
      "| id|firstname|middlename|lastname|dob_year|dob_month|gender|salary|lastname_starting_char|\n",
      "+---+---------+----------+--------+--------+---------+------+------+----------------------+\n",
      "|  2| Michael |      Rose|        |    2010|        3|     M|  4000|                  null|\n",
      "|  1|   James |          |   Smith|    2018|        1|     M|  3000|                     S|\n",
      "+---+---------+----------+--------+--------+---------+------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 저장된 데이터 로드하여 보여주기\n",
    "load_df = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(save_path)\n",
    "\n",
    "load_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7aece0786e0327c46fc2f6c6e3605b945939e6ae322a10dd0fa14e506f497c3b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
